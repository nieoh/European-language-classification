{
 "metadata": {
  "name": "",
  "signature": "sha256:182d7900a88ebe6e96389ca6ff3ccd183e46c1db959c48aadd7b5405625ce128"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Startup.ml Language Challenge"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Introduction"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "   In this challenge, we take the data from <a href=\"http://www.statmt.org/europarl/\"> the European Parliament Proceedings Parallel Corpus from 1996-2011</a>, to generate a language identification classifier to be used on the following <a href = \"https://storage.googleapis.com/google-code-archive-downloads/v2/code.google.com/language-detection/europarl-test.zip\">test set</a>. \n",
      "<br><br>\n",
      "   Initially we start by making some arbitrary decisions just to get the ball rolling on the challenge. I plan on continuing the project in a more proper manner, including better preprocessing of the data, as well as smarter choices for features in order to optimize the classifier. \n",
      "<br><br>\n",
      "   For now, we take a small subset of the training data and split it into training and testing sets and use a pipeline of TfIdf (term frequencey inverse document frequency) vectorizer and perceptron to create a simple classifier to attempt to identify a text as one of the 21 different languages."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Preprocess Data Files"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The initial data given consists of over 5 GB of text files. Before training the entire directory of txt, where simple mistakes and modification would be costly, we start with a smaller subset. I used the following bash command to create smaller subsets of each language data. In particular, use the following command to create a subfolder, and copy any file from the month of January. "
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "`for f in *; do mkdir ${f}_short; cp ${f}/ep-??-01-* ${f}_short; done`"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The resulting folders consist of 400-600 text files from the original data. Start the process with this subset. There wasn't a particular reason for choosing the month of January over any other criteria. "
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Load Short Data"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Start the analysis by loading the smaller dataset. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn.linear_model import Perceptron \n",
      "from sklearn.pipeline import Pipeline\n",
      "from sklearn.datasets import load_files\n",
      "from sklearn.cross_validation import train_test_split\n",
      "from sklearn import metrics\n",
      "\n",
      "#load training data\n",
      "txt_data_folder = '/Users/stephanieo/Desktop/MLProjects/European-language-classification/txt_short/'\n",
      "dataset = load_files(txt_data_folder, shuffle = True)\n",
      "\n",
      "#split data into test\n",
      "docs_train, docs_test, y_train, y_test = train_test_split(dataset.data, dataset.target, test_size = 0.5)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Classification: Tfidf Vectorizer + Perceptron"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Start by using a tfidf vectorizer to split the text into n-grams. Here, we use unigrams, bigrams and trigrams. It seems that language classification projects that use n-grams stick to n in the 1 to 5 range. Here I chose between 1 and 3 purely because I thought it would be a good compromise between accuracy and resources (time & having to listen to my computer get ready to take off). "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Vectorizer to split text into n-grams for n \\in {1, 2, 3}\n",
      "tfidf_vectorizer = TfidfVectorizer(ngram_range = (1, 3))\n",
      "\n",
      "#Pipeline using vectorizer and Perceptron\n",
      "clf = Pipeline([('vect', tfidf_vectorizer),\n",
      "                ('lin', Perceptron()),\n",
      "                ])\n",
      "\n",
      "#train data\n",
      "_ = clf.fit(docs_train, y_train)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now use the classifier to predict the test data we had split earlier. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "y_predicted = clf.predict(docs_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Testing on Short Data using train_test_split"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's look at the classification report."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print metrics.classification_report(y_test, y_predicted,\n",
      "                                    target_names=dataset.target_names)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "   bg_short       0.81      0.92      0.86       213\n",
      "   cs_short       0.99      0.89      0.94       280\n",
      "   da_short       0.96      0.94      0.95       276\n",
      "   de_short       0.91      0.94      0.92       295\n",
      "   el_short       0.99      0.95      0.97       262\n",
      "   en_short       1.00      0.94      0.97       304\n",
      "   es_short       1.00      0.93      0.96       288\n",
      "   et_short       1.00      0.93      0.96       298\n",
      "   fi_short       0.99      0.89      0.94       297\n",
      "   fr_short       0.91      0.95      0.93       293\n",
      "   hu_short       0.98      0.94      0.96       286\n",
      "   it_short       0.90      0.94      0.92       323\n",
      "   lt_short       1.00      0.90      0.94       266\n",
      "   lv_short       0.85      0.95      0.90       298\n",
      "   nl_short       0.99      0.93      0.96       299\n",
      "   pl_short       0.99      0.92      0.95       286\n",
      "   pt_short       0.72      0.96      0.82       294\n",
      "   ro_short       0.99      0.89      0.94       214\n",
      "   sk_short       0.95      0.90      0.93       325\n",
      "   sl_short       0.77      0.93      0.84       286\n",
      "   sv_short       1.00      0.94      0.97       313\n",
      "\n",
      "avg / total       0.94      0.93      0.93      5996"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "It seems that this super naive approach has a reasonable f1-score. Never having done any kind of language classifying other than the scikit-learn tutorial, I am pleasantly surprised. Of course, my test data is different form the acutal test data of the challenge so next steps would involve looking at a way to process the test data to run actual tests. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Initially when I ran this classifier, I did not expect to get such a \"high\" F-score so I didn't include a confusion matrix in the program. The second time around, I tried to pickle my classifier so I wouldn't have to re-train if I want to re-run my program to show some other metric, such as a plot of the confusion matrix. Unfortunately, pickling was taking a long time so this is another thing I would like to do in the future. But hypothetically, this is what you would do. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pickle\n",
      "\n",
      "s = pickle.dumps(clf)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So back to analyzing the classifier. Here is a plot of the confusion matrix. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print metrics.confusion_matrix(y_test, y_predicted)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "[[196   1   2   1   2   1   0   0   0   1   4   0   0   1   0   0   2   0   0   0   0]\n",
      " [  1 262   2   2   4   0   0   0   0   1   6   3   0   0   0   0   2   0   4   0   0]\n",
      " [  0   3 290   3   1   1   0   0   0   2   1   2   0   0   0   0   1   0   0   0   1]\n",
      " [  0   0   1 259   1   3   0   0   0   1   5   0   0   1   1   1   0   0   0   1   0]\n",
      " [  0   0   2   0 291   0   1   0   0   0   4   2   0   1   0   0   0   0   0   0   0]\n",
      " [  0   0   2   1   2 291   0   0   0   0   4   2   0   1   0   0   1   0   0   0   0]\n",
      " [  0   0   1   0   2   0 285   0   0   1   4   1   0   2   0   0   3   0   0   0   0]\n",
      " [  0   1   0   1   2   1   0 268   0   1   5   4   1   1   0   2   1   0   0   2   0]\n",
      " [  1   0   0   0   3   0   0   3 260   0   9   3   0   3   0   4   2   0   0   5   0]\n",
      " [  0   0   1   1   3   3   0   0   0 296   3   2   0   2   0   0   1   0   0   0   0]\n",
      " [  0   0   1   1   5   0   0   0   0   1 247   2   0   0   0   0   0   0   1   0   0]\n",
      " [  0   0   2   1   1   1   1   0   0   1   6 278   0   1   0   0   2   1   0   0   0]\n",
      " [  2   1   1   0   3   2   0   0   0   2   7   3 279   0   0   0   3   0   0   1   0]\n",
      " [  1   0   1   1   3   1   0   0   0   0   8   1   0 269   0   0   2   0   0   1   0]\n",
      " [  0   1   0   0   3   0   0   0   0   1   8   2   0   2 289   0   0   0   0   2   0]\n",
      " [  1   1   0   0   2   0   0   0   0   0   4   0   0   0   0 249   0   0   0   0   0]\n",
      " [  0   0   1   1   3   3   0   0   0   0   7   2   0   1   0   3 267   1   0   1   0]\n",
      " [  0   1   1   1   2   1   0   0   0   1   7   2   0   1   0   3   0 194   0   2   0]\n",
      " [  1   3   2   1   3   1   0   0   0   1   9   0   0   1   0   1   1   0 285   3   0]\n",
      " [  1   2   2   1   2   0   0   0   0   0   7   1   1   0   0   2   2   0   2 275   1]\n",
      " [  1   0   3   1   4   0   0   0   0   2   6   1   0   0   0   0   0   0   0   0 275]]\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The confusion matrix doesn't show a whole lot more than the precision and recall chart from above; however, I personally like the visual. "
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Testing on europarl.test"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Before I could submit this challenge, I got bored enough to parse through <a href = \"https://storage.googleapis.com/google-code-archive-downloads/v2/code.google.com/language-detection/europarl-test.zip\">europarl.test</a> using some basic bash commands, similar to the ones I used to preprocess the training data. \n",
      "<br><br>\n",
      "I proceeded to retrain the classifer using the whole short data training set, rather than the split version, and produced the following results when tested against europarl.test. \n",
      "<br><br>\n",
      "To start out with, simply add the path to the new testing dataset and rename all the variables to include all of the January data to the training set, rather than the split version. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Import test data\n",
      "test_data_folder = '/Users/stephanieo/Desktop/MLProjects/European-language-classification/test/'\n",
      "testset = load_files(test_data_folder)\n",
      "\n",
      "#Because I'm too lazy to change the rest of the variables\n",
      "docs_train, y_train = dataset.data, dataset.target\n",
      "docs_test, y_test = testset.data, testset.target "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Print the precision and recall grid and plot the confusion matrix for the new classifier against the actual testing data."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print metrics.classification_report(y_test, y_predicted,\n",
      "                                    target_names=dataset.target_names)\n",
      "\n",
      "print metrics.confusion_matrix(y_test, y_predicted)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "            precision    recall  f1-score   support\n",
      "\n",
      "   bg_short       1.00      0.88      0.93      1000\n",
      "   cs_short       1.00      0.70      0.82      1000\n",
      "   da_short       0.99      0.89      0.94      1000\n",
      "   de_short       0.98      0.93      0.95      1000\n",
      "   el_short       1.00      0.98      0.99      1000\n",
      "   en_short       0.94      0.95      0.94      1000\n",
      "   es_short       0.99      0.84      0.91      1000\n",
      "   et_short       0.99      0.81      0.89      1000\n",
      "   fi_short       0.97      0.80      0.88      1000\n",
      "   fr_short       0.94      0.90      0.92      1000\n",
      "   hu_short       1.00      0.79      0.89      1000\n",
      "   it_short       0.99      0.84      0.91      1000\n",
      "   lt_short       0.85      0.95      0.90      1000\n",
      "   lv_short       0.74      0.92      0.82      1000\n",
      "   nl_short       0.96      0.92      0.94      1000\n",
      "   pl_short       0.93      0.91      0.92      1000\n",
      "   pt_short       0.94      0.94      0.94      1000\n",
      "   ro_short       0.99      0.96      0.98      1000\n",
      "   sk_short       0.95      0.84      0.89      1000\n",
      "   sl_short       0.40      0.97      0.57      1000\n",
      "   sv_short       1.00      0.91      0.96      1000\n",
      "\n",
      "avg / total       0.93      0.89      0.90     21000\n",
      "\n",
      "\n",
      "[[876   0   0   0   0   0   0   0   0   1   0   0   0   2   0   0   0   0   0 121   0]\n",
      " [  0 698   0   0   0  12   1   0   0   0   0   0   2  19   1   6   1   0  15 245   0]\n",
      " [  0   0 890  17   0   1   1   0   0  14   0   1   6   6   4   0   4   2   3  51   0]\n",
      " [  0   0   0 928   0   0   0   0   1   1   0   0  13  12   0   2   7   0   1  35   0]\n",
      " [  0   0   0   0 981   0   0   0   0   0   0   0   0   0   0   0   0   0   0  19   0]\n",
      " [  0   0   1   0   0 947   0   0   0   0   0   0   1  10   0  18   1   0   0  22   0]\n",
      " [  0   0   0   0   0   0 843   0   0   0   0   0  10  74   4   1  31   0   0  37   0]\n",
      " [  0   0   0   0   0  28   0 810  21  24   0   0   5   5   0   0   0   0   0 107   0]\n",
      " [  0   0   0   0   0   1   0   1 800   0   0   0   8   3   1   0   0   0   0 186   0]\n",
      " [  1   0   0   0   0   0   1   0   0 896   0   4  16  57   2   3   1   3   1  15   0]\n",
      " [  0   0   0   0   0   0   1   9   0   1 794   0   9  19  18   0   0   0   0 149   0]\n",
      " [  0   0   0   1   0   0   4   0   0  10   0 840   5  40   1   1   3   1   2  92   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   3 954  14   0   0   0   0   0  29   0]\n",
      " [  0   0   0   0   0   0   0   0   4   0   0   0  61 918   1   0   1   0   0  15   0]\n",
      " [  0   0   1   1   0   2   0   0   0   0   0   0  12   7 920   8   0   0   0  49   0]\n",
      " [  0   0   0   0   0   7   0   0   0   0   0   0   0   4   0 907   0   0   9  73   0]\n",
      " [  0   0   1   0   0   1   0   0   0   0   0   0   6   6   0   7 942   0   5  32   0]\n",
      " [  0   0   0   0   0   0   0   0   0   2   0   0   0  10   0   0   2 962   0  24   0]\n",
      " [  0   1   0   0   0   8   0   0   1   0   0   0   7  20   0  18   0   0 843 102   0]\n",
      " [  0   1   0   0   0   1   0   0   0   3   0   0   2   5   1   1  10   0   1 975   0]\n",
      " [  0   0   8   0   0   0   1   0   1   0   0   0   8   3   2   0   1   0   4  58 914]]\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Surprisingly, the classifier doesn't seem to do too poorly against the actual test data either. The avg / total precision changed from 0.94 to 0.93; the recall from 0.93 to 0.89 and the F-score from 0.93 to 0.9. As you can see from the support, that the actual test set contained about 3.5 times more data points to be tested than our original test set, which has the potential to largely impact our F-score, yet it seems that the classifier fared well against a larger dataset. \n",
      "<br><br> There probably was some additional boost from training on twice the training data (about 6000 points to 12000 points). "
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Takeaways"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "There are a couple of academic and personal takeaways from working through this challenge. \n",
      "<br><br>\n",
      "I was pleasantly surprised at how easy it was to achieve reasonable accuracy using n-grams when classifying languages. There are a couple of easy things I would like to play around with to see the affect on the classifier:\n",
      "* Using different n-grams. Plot the change in the F-score as n gets larger.\n",
      "* Training on a larger dataset. This would most likely increase the accuracy of the classifier but at the expense of my computer. I should probably just sign up for a free AWS account and learn more about large computations.\n",
      "\n",
      "\n",
      "Moving forward, I would like to work out the kinks of this challenge, such as doing a proper preprocess of the data to strip it of any XML markings, as well as any empty spaces or punctuations. In addition, I would like to train the classifier on all the given data to see if this drastically improves the accuracy of the classifier when the same methods are used. My intuition is that a larger boost might come from using a combination of more training data and different algorithms but perhaps just sheer volume might do the trick.\n",
      "<br><br>\n",
      "In terms of language identification, as a native Korean speaker, I would find it interesting to see how well n-gram methods work with non-European languages. In particular, Korean is a langauge with an alphabet, but it would be interesting to see if there would be any kind of reasonable results for non-alphabet languages such as Chinese (I'm sure there are many papers out there about this). \n",
      "<br><br>\n",
      "Moving forward, I would personally like to learn more about the different metrics that can be used to judge a classifier, such as ROC vs. PR curves, F-scores, G-scores, the list goes on. Of course, I would like to expand my general knowledge of machine learning and find a niche for myself. Also, I'd like to refresh my memory and learn more about optimizing large computations so I don't have to sit here holding my laptop in front of a fan. "
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Thanks"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Regardless of the results of the application, I would like to thank Startup.ml and Erin Craig for reminding me of the joys of learning and working on a project. It is so easy to forget how much fun it is to learn something and lose track of time in a endless spiral of Wikipedia articles. "
     ]
    }
   ],
   "metadata": {}
  }
 ]
}
