{
 "metadata": {
  "name": "",
  "signature": "sha256:e51535afe976be25ade04f7abe88e49e16f7a350b84387b81368448defaba0fe"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Startup.ml Language Challenge"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Introduction"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "   In this challenge, we take the data from <a href=\"http://www.statmt.org/europarl/\"> the European Parliament Proceedings Parallel Corpus from 1996-2011 </a>, to generate a language identification classifier to be used on the following <a href = \"https://storage.googleapis.com/google-code-archive-downloads/v2/code.google.com/language-detection/europarl-test.zip\">test set</a>. \n",
      "<br><br>\n",
      "   Initially we start by making some arbitrary decisions just to get the ball rolling on the challenge. I plan on continuing the project in a more proper manner, including better preprocessing of the data, as well as smarter choices for features in order to optimize the classifier. \n",
      "<br><br>\n",
      "   For now, we take a small subset of the training data and split it into training and testing sets and use a pipeline of TfIdf (term frequencey inverse document frequency) vectorizer and perceptron to create a simple classifier to attempt to identify a text as one of the 21 different languages."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Preprocess Data Files"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The initial data given consists of over 5 GB of text files. Before training the entire directory of txt, where simple mistakes and modification would be costly, we start with a smaller subset. I used the following bash command to create smaller subsets of each language data. In particular, use the following command to create a subfolder, and copy any file from the month of January. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "for f in *; do mkdir ${f}_short; cp ${f}/ep-??-01-* ${f}_short; done"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The resulting folders consist of 400-600 text files from the original data. Start the process with this subset. There wasn't a particular reason for choosing the month of January over any other criteria. "
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Load Short Data"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Start the analysis by loading the smaller dataset. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn.linear_model import Perceptron \n",
      "from sklearn.pipeline import Pipeline\n",
      "from sklearn.datasets import load_files\n",
      "from sklearn.cross_validation import train_test_split\n",
      "from sklearn import metrics\n",
      "\n",
      "#load training data\n",
      "txt_data_folder = '/Users/stephanieo/Desktop/MLProjects/European-language-classification/txt_short/'\n",
      "dataset = load_files(txt_data_folder, shuffle = True)\n",
      "\n",
      "#split data into test\n",
      "docs_train, docs_test, y_train, y_test = train_test_split(dataset.data, dataset.target, test_size = 0.5)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Classification: Tfidf Vectorizer + Perceptron"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Start by using a tfidf vectorizer to split the text into n-grams. Here, we use unigrams, bigrams and trigrams. It seems that language classification projects that use n-grams stick to n in the 1 to 5 range. Here I chose between 1 and 3 purely because I thought it would be a good compromise between accuracy and resources (time & having to listen to my computer get ready to take off). "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Vectorizer to split text into n-grams for n \\in {1, 2, 3}\n",
      "tfidf_vectorizer = TfidfVectorizer(ngram_range = (1, 3))\n",
      "\n",
      "#Pipeline using vectorizer and Perceptron\n",
      "clf = Pipeline([('vect', tfidf_vectorizer),\n",
      "                ('lin', Perceptron()),\n",
      "                ])\n",
      "\n",
      "#train data\n",
      "_ = clf.fit(docs_train, y_train)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now use the classifier to predict the test data we had split earlier. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "y_predicted = clf.predict(docs_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's look at the classification report."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print metrics.classification_report(y_test, y_predicted,\n",
      "                                    target_names=dataset.target_names)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "   bg_short       0.81      0.92      0.86       213\n",
      "   cs_short       0.99      0.89      0.94       280\n",
      "   da_short       0.96      0.94      0.95       276\n",
      "   de_short       0.91      0.94      0.92       295\n",
      "   el_short       0.99      0.95      0.97       262\n",
      "   en_short       1.00      0.94      0.97       304\n",
      "   es_short       1.00      0.93      0.96       288\n",
      "   et_short       1.00      0.93      0.96       298\n",
      "   fi_short       0.99      0.89      0.94       297\n",
      "   fr_short       0.91      0.95      0.93       293\n",
      "   hu_short       0.98      0.94      0.96       286\n",
      "   it_short       0.90      0.94      0.92       323\n",
      "   lt_short       1.00      0.90      0.94       266\n",
      "   lv_short       0.85      0.95      0.90       298\n",
      "   nl_short       0.99      0.93      0.96       299\n",
      "   pl_short       0.99      0.92      0.95       286\n",
      "   pt_short       0.72      0.96      0.82       294\n",
      "   ro_short       0.99      0.89      0.94       214\n",
      "   sk_short       0.95      0.90      0.93       325\n",
      "   sl_short       0.77      0.93      0.84       286\n",
      "   sv_short       1.00      0.94      0.97       313\n",
      "\n",
      "avg / total       0.94      0.93      0.93      5996"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "It seems that this super naive approach has a reasonable f1-score. Never having done any kind of language classifying other than the scikit-learn tutorial, I am pleasantly surprised. Of course, my test data is different form the acutal test data of the challenge so next steps would involve looking at a way to process the test data to run actual tests. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Initially when I ran this classifier, I did not expect to get such a \"high\" F-score so I didn't include a confusion matrix in the program. The second time around, I tried to pickle my classifier so I wouldn't have to re-train if I want to re-run my program to show some other metric, such as a plot of the confusion matrix. Unfortunately, pickling was taking a long time so this is another thing I would like to do in the future. But hypothetically, this is what you would do. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pickle\n",
      "\n",
      "s = pickle.dumps(clf)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So back to analyzing the classifier. Here is a plot of the confusion matrix. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print metrics.confusion_matrix(y_test, y_predicted)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "[[196   1   2   1   2   1   0   0   0   1   4   0   0   1   0   0   2   0   0   0   0]\n",
      " [  1 262   2   2   4   0   0   0   0   1   6   3   0   0   0   0   2   0   4   0   0]\n",
      " [  0   3 290   3   1   1   0   0   0   2   1   2   0   0   0   0   1   0   0   0   1]\n",
      " [  0   0   1 259   1   3   0   0   0   1   5   0   0   1   1   1   0   0   0   1   0]\n",
      " [  0   0   2   0 291   0   1   0   0   0   4   2   0   1   0   0   0   0   0   0   0]\n",
      " [  0   0   2   1   2 291   0   0   0   0   4   2   0   1   0   0   1   0   0   0   0]\n",
      " [  0   0   1   0   2   0 285   0   0   1   4   1   0   2   0   0   3   0   0   0   0]\n",
      " [  0   1   0   1   2   1   0 268   0   1   5   4   1   1   0   2   1   0   0   2   0]\n",
      " [  1   0   0   0   3   0   0   3 260   0   9   3   0   3   0   4   2   0   0   5   0]\n",
      " [  0   0   1   1   3   3   0   0   0 296   3   2   0   2   0   0   1   0   0   0   0]\n",
      " [  0   0   1   1   5   0   0   0   0   1 247   2   0   0   0   0   0   0   1   0   0]\n",
      " [  0   0   2   1   1   1   1   0   0   1   6 278   0   1   0   0   2   1   0   0   0]\n",
      " [  2   1   1   0   3   2   0   0   0   2   7   3 279   0   0   0   3   0   0   1   0]\n",
      " [  1   0   1   1   3   1   0   0   0   0   8   1   0 269   0   0   2   0   0   1   0]\n",
      " [  0   1   0   0   3   0   0   0   0   1   8   2   0   2 289   0   0   0   0   2   0]\n",
      " [  1   1   0   0   2   0   0   0   0   0   4   0   0   0   0 249   0   0   0   0   0]\n",
      " [  0   0   1   1   3   3   0   0   0   0   7   2   0   1   0   3 267   1   0   1   0]\n",
      " [  0   1   1   1   2   1   0   0   0   1   7   2   0   1   0   3   0 194   0   2   0]\n",
      " [  1   3   2   1   3   1   0   0   0   1   9   0   0   1   0   1   1   0 285   3   0]\n",
      " [  1   2   2   1   2   0   0   0   0   0   7   1   1   0   0   2   2   0   2 275   1]\n",
      " [  1   0   3   1   4   0   0   0   0   2   6   1   0   0   0   0   0   0   0   0 275]]\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The confusion matrix doesn't show a whole lot more than the precision and recall chart from above; however, I personally like the visual. "
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Takeaways"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "There are a couple of academic and personal takeaways from working through this challenge. \n",
      "<br><br>\n",
      "I was pleasantly surprised at how easy it was to achieve reasonable accuracy using n-grams when classifying languages. There are a couple of easy things I would like to play around with to see the affect on the classifier:\n",
      "* Using different n-grams. Plot the change in the F-score as n gets larger.\n",
      "* Training on a larger dataset. This would most likely increase the accuracy of the classifier but at the expense of my computer. I should probably just sign up for a free AWS account and learn more about large computations.\n",
      "\n",
      "\n",
      "Moving forward, I would like to work out the kinks of this challenge, such as doing a proper preprocess of the data to strip it of any XML markings, as well as any empty spaces or punctuations. In addition, I would like to run the classifier against the actual test dataset to see what kind of F-score it achieves. \n",
      "<br><br>\n",
      "In terms of language identification, as a native Korean speaker, I would find it interesting to see how well n-gram methods work with non-European languages. In particular, Korean is a langauge with an alphabet, but it would be interesting to see if there would be any kind of reasonable results for non-alphabet languages such as Chinese (I'm sure there are many papers out there about this). \n",
      "<br><br>\n",
      "Moving forward, I would personally like to learn more about the different metrics that can be used to judge a classifier, such as ROC vs. PR curves, F-scores, G-scores, the list goes on. Of course, I would like to expand my general knowledge of machine learning and find a niche for myself. Also, I'd like to refresh my memory and learn more about optimizing large computations so I don't have to sit here holding my laptop in front of a fan. "
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Thanks"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Regardless of the results of the application, I would like to thank Startup.ml and Erin Craig for reminding me of the joys of learning and working on a project. It is so easy to forget how much fun it is to learn something and lose track of time in a endless spiral of Wikipedia articles. "
     ]
    }
   ],
   "metadata": {}
  }
 ]
}
